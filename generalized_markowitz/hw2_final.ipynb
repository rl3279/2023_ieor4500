{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Tuple\n",
    "def evalfunc(portfolio: np.ndarray, ret: np.ndarray, pi: float, theta: float) -> float:\n",
    "    \"\"\"\n",
    "    Task 1: the objective function\n",
    "    (Remember to vectorize as much as possible)\n",
    "\n",
    "\n",
    "    Parameters\n",
    "    --------------\n",
    "    portfolio: np.ndarray: the portfolio vector i.e. x\n",
    "\n",
    "    ret: np.ndarray: the (T, 3) numpy array containing all asset returns\n",
    "\n",
    "    pi: float: the exponent parameter of the objective\n",
    "\n",
    "    theta: the risk-aversion parameter of the objective\n",
    "\n",
    "\n",
    "    Returns\n",
    "    --------------\n",
    "    float: the objective value.\n",
    "    \"\"\"\n",
    "    # compute mean returns first. ret_mu shape should be (3,)\n",
    "    ret_mu = ret.mean(axis=0)\n",
    "\n",
    "    # first part\n",
    "    drift = -ret_mu.dot(portfolio)\n",
    "\n",
    "    # second part\n",
    "    # weighed deviation from mean (part within []^pi)\n",
    "    deviation = (ret - ret_mu).dot(portfolio)\n",
    "    risk = theta * (\n",
    "        (np.abs(deviation)**pi).mean()\n",
    "    )**(1/pi)\n",
    "    return drift + risk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following matrix notations, the objective function can be rewritten as\n",
    "\\begin{align*}\n",
    "\t\\text{minimize} \\ -\\bar{\\textbf{r}}'\\textbf{x}\n",
    "\t+\n",
    "\t\\frac{\\theta}{T^{1/\\pi}}\n",
    "\t\t||\n",
    "\t\t\t(\\textbf{r} - \\bar{\\textbf{r}})'\\textbf{x}\n",
    "\t\t||_{\\pi}\n",
    "\\end{align*}\n",
    "where $||\\cdot||^\\pi$ is the $L^\\pi$ norm. Here we must investigate how to take the derivative of a $L^p$ norm with respect to the function argument. According to\n",
    "[Wikipedia - Norm](https://en.wikipedia.org/wiki/Norm_(mathematics)),\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial ||\\textbf{x}||_p}{\\partial \\textbf{x}}&=\\frac{\\textbf{x}\\circ |\\textbf{x}|^{p-2}}{||\\textbf{x}||_p^{p-1}}\n",
    "\\end{align*}\n",
    "where $\\circ$ is element-wise matrix multiplication, and $|\\textbf{x}| = (|x_1|, \\ldots, |x_n|)$ is the element-wise absolute value. In other words, \n",
    "\\begin{align*}\n",
    "\\textbf{x}\\circ |\\textbf{x}|^{p-2}&=[x_1,\\ldots, x_n]\\circ [|x_1|^{p-2},\\ldots, |x_n|^{p-2}]\\\\\n",
    "&=[(x_1\\cdot |x_1|^{p-2}),\\ldots, (x_n\\cdot |x_n|^{p-2})]\n",
    "\\end{align*}\n",
    "Therefore, if we were to take derivative of the objective function with respect to $\\textbf{x}$ can be obtained via chain rule:\n",
    "\\begin{align*}\n",
    "\\frac{\\partial \\text{obj}}{\\partial \\textbf{x}}\n",
    "&=-\\bar{\\textbf{r}} + \\frac{\\theta}{T^{1/\\pi}}\\cdot  \\frac{[(\\textbf{r} - \\bar{\\textbf{r}})'\\textbf{x}]\\circ |(\\textbf{r} - \\bar{\\textbf{r}})'\\textbf{x}|^{\\pi -2}}{||(\\textbf{r} - \\bar{\\textbf{r}})'\\textbf{x}||_\\pi^{\\pi-1}}\\cdot (\\textbf{r}-\\bar{\\textbf{r}})\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalgrad(portfolio: np.ndarray, ret: np.ndarray, pi: float, theta: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Task 1: the objective function gradient\n",
    "\n",
    "\n",
    "    Parameters\n",
    "    --------------\n",
    "    portfolio: np.ndarray: the portfolio vector i.e. x\n",
    "\n",
    "    ret: np.ndarray: the (T, 3) numpy array containing all asset returns\n",
    "\n",
    "    pi: float: the exponent parameter of the objective\n",
    "\n",
    "    theta: the risk-aversion parameter of the objective\n",
    "\n",
    "\n",
    "    Returns\n",
    "    --------------\n",
    "    float: the objective gradient vector\n",
    "    \"\"\"\n",
    "    T = ret.shape[0]\n",
    "    ret_mu = ret.mean(axis=0)\n",
    "    delta = ret - ret_mu\n",
    "    dev = delta.dot(portfolio)\n",
    "    nom = dev * np.absolute(dev)**(pi-2)\n",
    "    # p-norm involves abs\n",
    "    denom = ((np.absolute(dev)**pi).sum())**(1-1/pi)\n",
    "    return -ret_mu + (\n",
    "        (theta / T**(1/pi))*nom/denom\n",
    "    ).dot(delta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Algorithm: Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtrack(\n",
    "        x:np.ndarray, ret:np.ndarray, \n",
    "        pi:float , theta:float, fval:float, grad:np.ndarray, delta:np.ndarray,\n",
    "        alpha:float = 0.5, beta:float = 0.75, step_eps:float=1e-4,\n",
    "        init_step:float = 1\n",
    "    ) -> Tuple[float, bool]:\n",
    "    \"\"\"\n",
    "    Task 1: backtrack step finder\n",
    "\n",
    "\n",
    "    Parameters\n",
    "    --------------\n",
    "    x: np.ndarray: portfolio weights\n",
    "\n",
    "    ret: np.ndarray: return matrix\n",
    "\n",
    "    pi: float: the exponent parameter of the objective\n",
    "\n",
    "    theta: the risk-aversion parameter of the objective\n",
    "\n",
    "    fval: float: current functional value\n",
    "\n",
    "    grad: np.ndarray: gradient\n",
    "\n",
    "    delta: np.ndarray: direction vector\n",
    "\n",
    "    alpha: float: acceptance threshold\n",
    "\n",
    "    beta: float: shrink ratio\n",
    "\n",
    "    step_eps: float: the tolerance lower bound of backtrack step\n",
    "\n",
    "    init_step: float: intial step size\n",
    "\n",
    "\n",
    "    Returns\n",
    "    --------------\n",
    "    Tuple[float, bool]: optimal step size, success boolean\n",
    "    \"\"\"\n",
    "    grad_dot_delta = grad.dot(delta)\n",
    "    step = init_step\n",
    "    goon = True\n",
    "    success = False\n",
    "    \n",
    "    while goon:\n",
    "        fnew = evalfunc(x + step * delta, ret, pi, theta)\n",
    "        target = alpha * step * grad_dot_delta\n",
    "\n",
    "        if fnew - fval <= target:\n",
    "            goon = False\n",
    "            success = True\n",
    "        else:\n",
    "            step *= beta\n",
    "        if step < step_eps:\n",
    "            goon = False\n",
    "    return step, success\n",
    "\n",
    "def get_descent(\n",
    "    step: float, \n",
    "    grad: np.ndarray,\n",
    "    momentum: bool = False,\n",
    "    olddelta: np.ndarray = None,\n",
    "    mu: float = None\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Task 1: helper function to generate the descent step,\n",
    "    for both momentum and non-momentum case.\n",
    "\n",
    "\n",
    "    Parameters\n",
    "    --------------\n",
    "    step: float: step size\n",
    "\n",
    "    grad: np.ndarray: gradient\n",
    "\n",
    "    momentum: bool: momentum flag\n",
    "\n",
    "    olddelta: np.ndarray: moving average vector for momentum\n",
    "\n",
    "    mu: float: conservation parameter for momentum descent\n",
    "\n",
    "    Returns\n",
    "    --------------\n",
    "    np.ndarray: descent step\n",
    "    \"\"\"\n",
    "    if not momentum:\n",
    "        # if not momentum just negative gradient direction times step size\n",
    "        return - step * grad\n",
    "    else:\n",
    "        # if momentum the convex combination of moving average and conventional step\n",
    "        assert (olddelta is not None) and (mu is not None)\n",
    "        return - step * grad + (1-mu) * olddelta\n",
    "\n",
    "\n",
    "\n",
    "def run_grad_desc(\n",
    "    x: np.ndarray,\n",
    "    ret: np.ndarray,\n",
    "    pi: float, \n",
    "    theta: float,\n",
    "    x_history: np.ndarray,\n",
    "    f_history: np.ndarray,\n",
    "    bt: bool = True,\n",
    "    bt_a: float = None,\n",
    "    bt_b: float = None,\n",
    "    bt_init_step: float = 1,\n",
    "    momentum: bool = False,\n",
    "    mom_mu: float = None,\n",
    "    max_iter: int = 1000,\n",
    "    step_eps: float = 0.05,\n",
    ") -> Tuple[bool, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Task 1: Main GD algorithm. Incorporates the possibility to toggle\n",
    "    both the momentum flag and the backtrack flag.\n",
    "\n",
    "\n",
    "    Parameters\n",
    "    --------------\n",
    "    x: np.ndarray: portfolio weights\n",
    "\n",
    "    ret: np.ndarray: return matrix\n",
    "\n",
    "    pi: float: the exponent parameter of the objective\n",
    "\n",
    "    theta: float: the risk-aversion parameter of the objective\n",
    "\n",
    "    x_history: np.ndarray: portfolio weight vector\n",
    "\n",
    "    f_history: np.ndarray: objective vector\n",
    "\n",
    "    bt: bool: backtrack flag. If False, use constant step specified by step_eps.\n",
    "\n",
    "    bt_a: float: backtrack acceptance threshold\n",
    "\n",
    "    bt_b: float: backtrack shrink ratio\n",
    "\n",
    "    bt_init_step: float: backtrack initial step size, default 1.\n",
    "\n",
    "    momentum: bool: momentum flag\n",
    "\n",
    "    mom_mu: float: momentum conservation parameter\n",
    "\n",
    "    max_iter: int: iteration limit\n",
    "\n",
    "    step_eps: float: the tolerance lower bound of backtrack step if bt=True,\n",
    "    the step size if bt=False.\n",
    "    \"\"\"\n",
    "    converged = False\n",
    "    iter = 0\n",
    "    descent = np.zeros_like(x)\n",
    "    while iter < max_iter:\n",
    "        x_history[iter] = x\n",
    "        fval = evalfunc(x, ret, pi, theta)\n",
    "        grad = evalgrad(x, ret, pi, theta)\n",
    "        f_history[iter] = fval  \n",
    "        if bt:\n",
    "            # if backtrack, call backtrack function to compute step size\n",
    "            step, goodstep = backtrack(x, ret, pi, theta, fval, grad, -grad, bt_a, bt_b, step_eps, bt_init_step)\n",
    "            goodstep = True\n",
    "        else:\n",
    "            # if not, use constant step size\n",
    "            goodstep = True\n",
    "            step = step_eps\n",
    "\n",
    "        # one line to compute descent for both momentum and non-momentum case\n",
    "        descent = get_descent(step, grad, momentum, descent, mom_mu)\n",
    "        if goodstep:\n",
    "            # if good step, descend\n",
    "            x += descent\n",
    "            \n",
    "            if np.isclose(grad, 0).all():\n",
    "                converged = True\n",
    "                print(\"Converged. x:\", x)\n",
    "                break\n",
    "            else:\n",
    "                pass\n",
    "                # if iter % 10 == 0:\n",
    "                #     print(f\"grad {iter} = {grad}\")\n",
    "                #     print(f\"grad L2 {iter} = {np.inner(grad, grad)}\")\n",
    "        iter += 1\n",
    "    return iter, converged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-e2922964de43>:77: DtypeWarning: Columns (0) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  read_asset(\"AMZN\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AMZN_ret</th>\n",
       "      <th>AMZN_price</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-01-04</th>\n",
       "      <td>-0.027139</td>\n",
       "      <td>3262.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-05</th>\n",
       "      <td>0.006993</td>\n",
       "      <td>3174.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-06</th>\n",
       "      <td>0.010673</td>\n",
       "      <td>3146.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-07</th>\n",
       "      <td>0.010303</td>\n",
       "      <td>3162.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-08</th>\n",
       "      <td>-0.005051</td>\n",
       "      <td>3173.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-07-07</th>\n",
       "      <td>-0.004655</td>\n",
       "      <td>3729.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-07-08</th>\n",
       "      <td>0.004955</td>\n",
       "      <td>3652.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-07-09</th>\n",
       "      <td>0.002678</td>\n",
       "      <td>3718.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-07-12</th>\n",
       "      <td>-0.011032</td>\n",
       "      <td>3746.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-07-13</th>\n",
       "      <td>0.011858</td>\n",
       "      <td>3708.82</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>130 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            AMZN_ret  AMZN_price\n",
       "Date                            \n",
       "2021-01-04 -0.027139     3262.80\n",
       "2021-01-05  0.006993     3174.80\n",
       "2021-01-06  0.010673     3146.17\n",
       "2021-01-07  0.010303     3162.20\n",
       "2021-01-08 -0.005051     3173.77\n",
       "...              ...         ...\n",
       "2021-07-07 -0.004655     3729.24\n",
       "2021-07-08  0.004955     3652.90\n",
       "2021-07-09  0.002678     3718.54\n",
       "2021-07-12 -0.011032     3746.21\n",
       "2021-07-13  0.011858     3708.82\n",
       "\n",
       "[130 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "def is_open_or_noon(dt: datetime.datetime) -> bool:\n",
    "    \"\"\"Task 1: read helper function for datetime parsing\"\"\"\n",
    "    t = dt.time()\n",
    "    return t == datetime.time(9,30) or t == datetime.time(12,0)\n",
    "\n",
    "def is_open(dt:datetime.datetime) -> bool:\n",
    "    \"\"\"Task 1: read helper function for datetime parsing\"\"\"\n",
    "    return dt.time() == datetime.time(9,30)\n",
    "\n",
    "def my_dt_parser(s: str) -> datetime.datetime:\n",
    "    \"\"\"Task 1: custom datetime parser. 5x faster than pd.to_datetime\"\"\"\n",
    "    date, time = s.split()\n",
    "    m, d, y = date.split(\"/\")\n",
    "    H, M = time.split(\":\")\n",
    "    return datetime.datetime(\n",
    "        year = 2000 + int(y),\n",
    "        month = int(m),\n",
    "        day = int(d),\n",
    "        hour = int(H),\n",
    "        minute = int(M)\n",
    "    )\n",
    "\n",
    "def read_asset(asset:str, data_dir: str=\"../data/\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Task 1: reads a single asset.\n",
    "\n",
    "\n",
    "    Parameters\n",
    "    --------------\n",
    "    asset: str: asset name\n",
    "\n",
    "    data_dir: str: local data directory\n",
    "\n",
    "\n",
    "    Returns\n",
    "    --------------\n",
    "    pd.DataFrame: pandas dataframe containing asset returns and price\n",
    "    \"\"\"\n",
    "    # static var to mark missing data\n",
    "    missing = -999.\n",
    "    # read csv\n",
    "    csv_path = data_dir + asset + \".csv\"\n",
    "    df = pd.read_csv(csv_path, header=3).loc[:, [\"Dates\", \"Close\"]]\n",
    "    # read up to empty entries\n",
    "    df = df.iloc[:df[\"Close\"].isna().argmax()]\n",
    "\n",
    "    # deal with first row missing date\n",
    "    df.loc[0,\"Dates\"] = df.loc[1,\"Dates\"].replace(\"31\", \"30\")\n",
    "\n",
    "    # extract open or noon data\n",
    "    df[\"dt\"] = df[\"Dates\"].apply(my_dt_parser)\n",
    "    df[\"Date\"] = df[\"dt\"].apply(lambda dt: dt.date())\n",
    "    open_or_noon = df[\"dt\"].apply(is_open_or_noon)\n",
    "    df = df.loc[open_or_noon]\n",
    "\n",
    "    # compute daily return\n",
    "    ret = df.loc[:, [\"Close\",\"Date\"]].groupby(\"Date\").apply(\n",
    "        lambda x: x[\"Close\"].iloc[1]/x[\"Close\"].iloc[0]-1 if len(x) == 2 else missing\n",
    "    ).values\n",
    "\n",
    "    ret = ret[~np.isnan(ret)]\n",
    "\n",
    "    # return along with daily open price\n",
    "    df = df.loc[df[\"dt\"].apply(is_open)]\n",
    "    df[\"ret\"] = ret\n",
    "\n",
    "    # filter out bad dates with missing data (particularly at noon)\n",
    "    df = df.loc[df.ret > missing + 1]\n",
    "    df.set_index(\"Date\", inplace=True)\n",
    "    df = df[[\"ret\", \"Close\"]]\n",
    "    df.rename(columns = {\"ret\": f\"{asset}_ret\", \"Close\": f\"{asset}_price\"}, inplace = True)\n",
    "    return df\n",
    "\n",
    "read_asset(\"AMZN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-2526a8c13171>:19: DtypeWarning: Columns (0) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  dfs = [read_asset(asset, data_dir) for asset in assets]\n"
     ]
    }
   ],
   "source": [
    "def read_all(data_dir: str = \"../data/\", T: int = 100) -> Tuple[pd.DataFrame,pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Task 1.5: reads all asset returns and sever into train and test.\n",
    "\n",
    "\n",
    "    Parameters\n",
    "    --------------\n",
    "    data_dir: str: local data directory\n",
    "\n",
    "    T: int: size of the traning period.\n",
    "\n",
    "\n",
    "    Returns\n",
    "    --------------\n",
    "    Tuple[pd.DataFrame, pd.DataFrame]: train and test pandas dataframe containing returns and prices\n",
    "    \"\"\"\n",
    "    assets = [\"AMZN\", \"NFLX\", \"TSLA\"]\n",
    "\n",
    "    dfs = [read_asset(asset, data_dir) for asset in assets]\n",
    "\n",
    "    df = dfs[0]\n",
    "    for i in range(1, 3):\n",
    "        df = df.join(dfs[i])\n",
    "    df = df.dropna()\n",
    "    return df.iloc[:T], df.iloc[T:]\n",
    "\n",
    "train, test = read_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split\n",
    "ret_train = train.loc[:,[col for col in train.columns if \"ret\" in col]]\n",
    "price_train = train.loc[:,[col for col in train.columns if \"price\" in col]]\n",
    "ret_test = test.loc[:,[col for col in test.columns if \"ret\" in col]].values\n",
    "price_test= test.loc[:,[col for col in test.columns if \"price\" in col]].values\n",
    "\n",
    "# Task 2\n",
    "def benchmark(ret_test: np.ndarray, price_test: np.ndarray, x: np.ndarray):\n",
    "    \"\"\"Task 2: Benchmarking\n",
    "    \n",
    "    \n",
    "    Parameters\n",
    "    --------------\n",
    "    ret_test: np.ndarray: test return matrix\n",
    "\n",
    "    price_test: np.ndarray: test price matrix\n",
    "\n",
    "    x: np.ndarray: portfolio weights (normalized)\n",
    "    \"\"\"\n",
    "    p0 = 1e9\n",
    "    shares = p0 / np.abs(x).sum() * x / price_test\n",
    "    portfolio_return = ret_test.dot(x).mean()\n",
    "    return shares, portfolio_return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Run\n",
    "\n",
    "$\\pi = 2$, $\\theta = 10^{-6}$ for $10000$ iterations. \n",
    "\n",
    "Expected behavior: since almost no risk aversion is factored into the objective, the objective and weights should diverge to negative infinity since all three assets have negative mean returns in training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged: False, after 10000 iterations\n"
     ]
    }
   ],
   "source": [
    "ret = ret_train.values\n",
    "pi = 2; theta = 1e-6; portfolio = np.array([1e-3]*3)\n",
    "max_iter = 10000\n",
    "m = len(portfolio)\n",
    "x_history = np.zeros((max_iter, m))\n",
    "f_history = np.zeros(max_iter)\n",
    "it, converged = run_grad_desc(\n",
    "    x = portfolio, \n",
    "    ret = ret, \n",
    "    pi = pi, theta = theta, x_history = x_history, f_history = f_history,\n",
    "    bt=True, bt_a=0.5, bt_b=0.75, bt_init_step=1,\n",
    "    momentum=False, mom_mu=0.8, \n",
    "    max_iter = max_iter, step_eps=1e-4, \n",
    ")\n",
    "\n",
    "print(f\"Converged: {str(converged)}, after {it} iterations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final objective = -0.03885\n",
      "Final portfolio = [-2.57826, -3.4718, -19.22731]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Pi = 2, Theta = 1e-06,\\nFinal portfolio = [-2.57826, -3.4718, -19.22731]')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize= (15, 5))\n",
    "\n",
    "ax[0].plot(f_history[:it])\n",
    "ax[1].plot(x_history[:it])\n",
    "\n",
    "final_objective = round(f_history[it-1],5)\n",
    "final_portfolio = [round(p,5) for p in x_history[it-1]]\n",
    "print(f\"Final objective = {final_objective}\")\n",
    "print(f\"Final portfolio = {final_portfolio}\")\n",
    "\n",
    "ax[0].set_title(f\"Pi = {pi}, Theta = {theta},\\nFinal objective = {final_objective} at iter {it}.\")\n",
    "ax[1].set_title(f\"Pi = {pi}, Theta = {theta},\\nFinal portfolio = {final_portfolio}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shares to be traded for each day (AMZN, NFLX, TSLA respectively):\n",
      "[[  -29824.28145445  -259049.69775212 -1131907.42090212]\n",
      " [  -29656.83259134  -258338.31724819 -1110523.55050797]\n",
      " [  -29608.19295821  -257727.51960552 -1114591.70037429]\n",
      " [  -29647.00571188  -260890.00066135 -1110935.51871259]\n",
      " [  -29562.11143644  -255655.10711819 -1116866.5109101 ]\n",
      " [  -28735.03557593  -257471.45889619 -1112423.74143794]\n",
      " [  -27351.080738    -253171.64614141 -1147947.70142832]\n",
      " [  -27922.67632057  -258299.45038585 -1210959.1730526 ]\n",
      " [  -27429.78274575  -258976.43018417 -1159196.43079052]\n",
      " [  -27227.18276108  -255142.20119664 -1145729.27960927]\n",
      " [  -27501.67016231  -255774.12940311 -1104892.22158534]]\n",
      "\n",
      "Final mean portfolio returns:\n",
      "0.0026382216821722554\n"
     ]
    }
   ],
   "source": [
    "shares, ret_bar = benchmark(ret_test, price_test, np.array(final_portfolio))\n",
    "print(\"Shares to be traded for each day (AMZN, NFLX, TSLA respectively):\")\n",
    "print(shares)\n",
    "print()\n",
    "print(\"Final mean portfolio returns:\")\n",
    "print(ret_bar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************************\n",
      "RUNNING: theta=0.1, pi=0.5, bt=True, mom=True...\n",
      "\tstep_eps: 0.0001\n",
      "************************************************************\n",
      "\n",
      "************************************************************\n",
      "RUNNING: theta=0.1, pi=0.5, bt=True, mom=False...\n",
      "\tstep_eps: 0.0001\n",
      "************************************************************\n",
      "\n",
      "************************************************************\n",
      "RUNNING: theta=0.1, pi=0.5, bt=False, mom=True...\n",
      "\tstep_eps: 0.1\n",
      "************************************************************\n",
      "\n",
      "\tstep_eps: 0.01\n",
      "************************************************************\n",
      "\n",
      "\tstep_eps: 1\n",
      "************************************************************\n",
      "\n",
      "************************************************************\n",
      "RUNNING: theta=0.1, pi=0.5, bt=False, mom=False...\n",
      "\tstep_eps: 0.1\n",
      "************************************************************\n",
      "\n",
      "\tstep_eps: 0.01\n",
      "************************************************************\n",
      "\n",
      "\tstep_eps: 1\n",
      "************************************************************\n",
      "\n",
      "************************************************************\n",
      "RUNNING: theta=0.1, pi=2, bt=True, mom=True...\n",
      "\tstep_eps: 0.0001\n",
      "************************************************************\n",
      "\n",
      "************************************************************\n",
      "RUNNING: theta=0.1, pi=2, bt=True, mom=False...\n",
      "\tstep_eps: 0.0001\n",
      "************************************************************\n",
      "\n",
      "************************************************************\n",
      "RUNNING: theta=0.1, pi=2, bt=False, mom=True...\n",
      "\tstep_eps: 0.1\n",
      "************************************************************\n",
      "\n",
      "\tstep_eps: 0.01\n",
      "************************************************************\n",
      "\n",
      "\tstep_eps: 1\n",
      "************************************************************\n",
      "\n",
      "************************************************************\n",
      "RUNNING: theta=0.1, pi=2, bt=False, mom=False...\n",
      "\tstep_eps: 0.1\n",
      "************************************************************\n",
      "\n",
      "\tstep_eps: 0.01\n",
      "************************************************************\n",
      "\n",
      "\tstep_eps: 1\n",
      "************************************************************\n",
      "\n",
      "************************************************************\n",
      "RUNNING: theta=0.1, pi=4, bt=True, mom=True...\n",
      "\tstep_eps: 0.0001\n",
      "************************************************************\n",
      "\n",
      "************************************************************\n",
      "RUNNING: theta=0.1, pi=4, bt=True, mom=False...\n",
      "\tstep_eps: 0.0001\n",
      "************************************************************\n",
      "\n",
      "************************************************************\n",
      "RUNNING: theta=0.1, pi=4, bt=False, mom=True...\n",
      "\tstep_eps: 0.1\n",
      "************************************************************\n",
      "\n",
      "\tstep_eps: 0.01\n",
      "************************************************************\n",
      "\n",
      "\tstep_eps: 1\n",
      "************************************************************\n",
      "\n",
      "************************************************************\n",
      "RUNNING: theta=0.1, pi=4, bt=False, mom=False...\n",
      "\tstep_eps: 0.1\n",
      "************************************************************\n",
      "\n",
      "\tstep_eps: 0.01\n",
      "************************************************************\n",
      "\n",
      "\tstep_eps: 1\n",
      "************************************************************\n",
      "\n",
      "************************************************************\n",
      "RUNNING: theta=0.1, pi=6, bt=True, mom=True...\n",
      "\tstep_eps: 0.0001\n",
      "************************************************************\n",
      "\n",
      "************************************************************\n",
      "RUNNING: theta=0.1, pi=6, bt=True, mom=False...\n",
      "\tstep_eps: 0.0001\n",
      "************************************************************\n",
      "\n",
      "************************************************************\n",
      "RUNNING: theta=0.1, pi=6, bt=False, mom=True...\n",
      "\tstep_eps: 0.1\n",
      "************************************************************\n",
      "\n",
      "\tstep_eps: 0.01\n",
      "************************************************************\n",
      "\n",
      "\tstep_eps: 1\n",
      "************************************************************\n",
      "\n",
      "************************************************************\n",
      "RUNNING: theta=0.1, pi=6, bt=False, mom=False...\n",
      "\tstep_eps: 0.1\n",
      "************************************************************\n",
      "\n",
      "\tstep_eps: 0.01\n",
      "************************************************************\n",
      "\n",
      "\tstep_eps: 1\n",
      "************************************************************\n",
      "\n",
      "************************************************************\n",
      "RUNNING: theta=10, pi=0.5, bt=True, mom=True...\n",
      "\tstep_eps: 0.0001\n",
      "************************************************************\n",
      "\n",
      "************************************************************\n",
      "RUNNING: theta=10, pi=0.5, bt=True, mom=False...\n",
      "\tstep_eps: 0.0001\n",
      "************************************************************\n",
      "\n",
      "************************************************************\n",
      "RUNNING: theta=10, pi=0.5, bt=False, mom=True...\n",
      "\tstep_eps: 0.1\n",
      "************************************************************\n",
      "\n",
      "\tstep_eps: 0.01\n",
      "************************************************************\n",
      "\n",
      "\tstep_eps: 1\n",
      "************************************************************\n",
      "\n",
      "************************************************************\n",
      "RUNNING: theta=10, pi=0.5, bt=False, mom=False...\n",
      "\tstep_eps: 0.1\n",
      "************************************************************\n",
      "\n",
      "\tstep_eps: 0.01\n",
      "************************************************************\n",
      "\n",
      "\tstep_eps: 1\n",
      "************************************************************\n",
      "\n",
      "************************************************************\n",
      "RUNNING: theta=10, pi=2, bt=True, mom=True...\n",
      "\tstep_eps: 0.0001\n",
      "************************************************************\n",
      "\n",
      "************************************************************\n",
      "RUNNING: theta=10, pi=2, bt=True, mom=False...\n",
      "\tstep_eps: 0.0001\n",
      "************************************************************\n",
      "\n",
      "************************************************************\n",
      "RUNNING: theta=10, pi=2, bt=False, mom=True...\n",
      "\tstep_eps: 0.1\n",
      "************************************************************\n",
      "\n",
      "\tstep_eps: 0.01\n",
      "************************************************************\n",
      "\n",
      "\tstep_eps: 1\n",
      "************************************************************\n",
      "\n",
      "************************************************************\n",
      "RUNNING: theta=10, pi=2, bt=False, mom=False...\n",
      "\tstep_eps: 0.1\n",
      "************************************************************\n",
      "\n",
      "\tstep_eps: 0.01\n",
      "************************************************************\n",
      "\n",
      "\tstep_eps: 1\n",
      "************************************************************\n",
      "\n",
      "************************************************************\n",
      "RUNNING: theta=10, pi=4, bt=True, mom=True...\n",
      "\tstep_eps: 0.0001\n",
      "************************************************************\n",
      "\n",
      "************************************************************\n",
      "RUNNING: theta=10, pi=4, bt=True, mom=False...\n",
      "\tstep_eps: 0.0001\n",
      "************************************************************\n",
      "\n",
      "************************************************************\n",
      "RUNNING: theta=10, pi=4, bt=False, mom=True...\n",
      "\tstep_eps: 0.1\n",
      "************************************************************\n",
      "\n",
      "\tstep_eps: 0.01\n",
      "************************************************************\n",
      "\n",
      "\tstep_eps: 1\n",
      "************************************************************\n",
      "\n",
      "************************************************************\n",
      "RUNNING: theta=10, pi=4, bt=False, mom=False...\n",
      "\tstep_eps: 0.1\n",
      "************************************************************\n",
      "\n",
      "\tstep_eps: 0.01\n",
      "************************************************************\n",
      "\n",
      "\tstep_eps: 1\n",
      "************************************************************\n",
      "\n",
      "************************************************************\n",
      "RUNNING: theta=10, pi=6, bt=True, mom=True...\n",
      "\tstep_eps: 0.0001\n",
      "************************************************************\n",
      "\n",
      "************************************************************\n",
      "RUNNING: theta=10, pi=6, bt=True, mom=False...\n",
      "\tstep_eps: 0.0001\n",
      "************************************************************\n",
      "\n",
      "************************************************************\n",
      "RUNNING: theta=10, pi=6, bt=False, mom=True...\n",
      "\tstep_eps: 0.1\n",
      "************************************************************\n",
      "\n",
      "\tstep_eps: 0.01\n",
      "************************************************************\n",
      "\n",
      "\tstep_eps: 1\n",
      "************************************************************\n",
      "\n",
      "************************************************************\n",
      "RUNNING: theta=10, pi=6, bt=False, mom=False...\n",
      "\tstep_eps: 0.1\n",
      "************************************************************\n",
      "\n",
      "\tstep_eps: 0.01\n",
      "************************************************************\n",
      "\n",
      "\tstep_eps: 1\n",
      "************************************************************\n",
      "\n",
      "************************************************************\n",
      "RUNNING: theta=1000, pi=0.5, bt=True, mom=True...\n",
      "\tstep_eps: 0.0001\n",
      "************************************************************\n",
      "\n",
      "************************************************************\n",
      "RUNNING: theta=1000, pi=0.5, bt=True, mom=False...\n",
      "\tstep_eps: 0.0001\n",
      "************************************************************\n",
      "\n",
      "************************************************************\n",
      "RUNNING: theta=1000, pi=0.5, bt=False, mom=True...\n",
      "\tstep_eps: 0.1\n",
      "************************************************************\n",
      "\n",
      "\tstep_eps: 0.01\n",
      "************************************************************\n",
      "\n",
      "\tstep_eps: 1\n",
      "************************************************************\n",
      "\n",
      "************************************************************\n",
      "RUNNING: theta=1000, pi=0.5, bt=False, mom=False...\n",
      "\tstep_eps: 0.1\n",
      "************************************************************\n",
      "\n",
      "\tstep_eps: 0.01\n",
      "************************************************************\n",
      "\n",
      "\tstep_eps: 1\n",
      "************************************************************\n",
      "\n",
      "************************************************************\n",
      "RUNNING: theta=1000, pi=2, bt=True, mom=True...\n",
      "\tstep_eps: 0.0001\n",
      "************************************************************\n",
      "\n",
      "************************************************************\n",
      "RUNNING: theta=1000, pi=2, bt=True, mom=False...\n",
      "\tstep_eps: 0.0001\n",
      "************************************************************\n",
      "\n",
      "************************************************************\n",
      "RUNNING: theta=1000, pi=2, bt=False, mom=True...\n",
      "\tstep_eps: 0.1\n",
      "************************************************************\n",
      "\n",
      "\tstep_eps: 0.01\n",
      "************************************************************\n",
      "\n",
      "\tstep_eps: 1\n",
      "************************************************************\n",
      "\n",
      "************************************************************\n",
      "RUNNING: theta=1000, pi=2, bt=False, mom=False...\n",
      "\tstep_eps: 0.1\n",
      "************************************************************\n",
      "\n",
      "\tstep_eps: 0.01\n",
      "************************************************************\n",
      "\n",
      "\tstep_eps: 1\n",
      "************************************************************\n",
      "\n",
      "************************************************************\n",
      "RUNNING: theta=1000, pi=4, bt=True, mom=True...\n",
      "\tstep_eps: 0.0001\n",
      "************************************************************\n",
      "\n",
      "************************************************************\n",
      "RUNNING: theta=1000, pi=4, bt=True, mom=False...\n",
      "\tstep_eps: 0.0001\n",
      "************************************************************\n",
      "\n",
      "************************************************************\n",
      "RUNNING: theta=1000, pi=4, bt=False, mom=True...\n",
      "\tstep_eps: 0.1\n",
      "************************************************************\n",
      "\n",
      "\tstep_eps: 0.01\n",
      "************************************************************\n",
      "\n",
      "\tstep_eps: 1\n",
      "************************************************************\n",
      "\n",
      "************************************************************\n",
      "RUNNING: theta=1000, pi=4, bt=False, mom=False...\n",
      "\tstep_eps: 0.1\n",
      "************************************************************\n",
      "\n",
      "\tstep_eps: 0.01\n",
      "************************************************************\n",
      "\n",
      "\tstep_eps: 1\n",
      "************************************************************\n",
      "\n",
      "************************************************************\n",
      "RUNNING: theta=1000, pi=6, bt=True, mom=True...\n",
      "\tstep_eps: 0.0001\n",
      "************************************************************\n",
      "\n",
      "************************************************************\n",
      "RUNNING: theta=1000, pi=6, bt=True, mom=False...\n",
      "\tstep_eps: 0.0001\n",
      "************************************************************\n",
      "\n",
      "************************************************************\n",
      "RUNNING: theta=1000, pi=6, bt=False, mom=True...\n",
      "\tstep_eps: 0.1\n",
      "************************************************************\n",
      "\n",
      "\tstep_eps: 0.01\n",
      "************************************************************\n",
      "\n",
      "\tstep_eps: 1\n",
      "************************************************************\n",
      "\n",
      "************************************************************\n",
      "RUNNING: theta=1000, pi=6, bt=False, mom=False...\n",
      "\tstep_eps: 0.1\n",
      "************************************************************\n",
      "\n",
      "\tstep_eps: 0.01\n",
      "************************************************************\n",
      "\n",
      "\tstep_eps: 1\n",
      "************************************************************\n",
      "\n",
      "************************************************************\n",
      "RUNNING: theta=100000, pi=0.5, bt=True, mom=True...\n",
      "\tstep_eps: 0.0001\n",
      "************************************************************\n",
      "\n",
      "************************************************************\n",
      "RUNNING: theta=100000, pi=0.5, bt=True, mom=False...\n",
      "\tstep_eps: 0.0001\n",
      "************************************************************\n",
      "\n",
      "************************************************************\n",
      "RUNNING: theta=100000, pi=0.5, bt=False, mom=True...\n",
      "\tstep_eps: 0.1\n",
      "************************************************************\n",
      "\n",
      "\tstep_eps: 0.01\n",
      "************************************************************\n",
      "\n",
      "\tstep_eps: 1\n",
      "************************************************************\n",
      "\n",
      "************************************************************\n",
      "RUNNING: theta=100000, pi=0.5, bt=False, mom=False...\n",
      "\tstep_eps: 0.1\n",
      "************************************************************\n",
      "\n",
      "\tstep_eps: 0.01\n",
      "************************************************************\n",
      "\n",
      "\tstep_eps: 1\n",
      "************************************************************\n",
      "\n",
      "************************************************************\n",
      "RUNNING: theta=100000, pi=2, bt=True, mom=True...\n",
      "\tstep_eps: 0.0001\n",
      "************************************************************\n",
      "\n",
      "************************************************************\n",
      "RUNNING: theta=100000, pi=2, bt=True, mom=False...\n",
      "\tstep_eps: 0.0001\n",
      "************************************************************\n",
      "\n",
      "************************************************************\n",
      "RUNNING: theta=100000, pi=2, bt=False, mom=True...\n",
      "\tstep_eps: 0.1\n",
      "************************************************************\n",
      "\n",
      "\tstep_eps: 0.01\n",
      "************************************************************\n",
      "\n",
      "\tstep_eps: 1\n",
      "************************************************************\n",
      "\n",
      "************************************************************\n",
      "RUNNING: theta=100000, pi=2, bt=False, mom=False...\n",
      "\tstep_eps: 0.1\n",
      "************************************************************\n",
      "\n",
      "\tstep_eps: 0.01\n",
      "************************************************************\n",
      "\n",
      "\tstep_eps: 1\n",
      "************************************************************\n",
      "\n",
      "************************************************************\n",
      "RUNNING: theta=100000, pi=4, bt=True, mom=True...\n",
      "\tstep_eps: 0.0001\n",
      "************************************************************\n",
      "\n",
      "************************************************************\n",
      "RUNNING: theta=100000, pi=4, bt=True, mom=False...\n",
      "\tstep_eps: 0.0001\n",
      "************************************************************\n",
      "\n",
      "************************************************************\n",
      "RUNNING: theta=100000, pi=4, bt=False, mom=True...\n",
      "\tstep_eps: 0.1\n",
      "************************************************************\n",
      "\n",
      "\tstep_eps: 0.01\n",
      "************************************************************\n",
      "\n",
      "\tstep_eps: 1\n",
      "************************************************************\n",
      "\n",
      "************************************************************\n",
      "RUNNING: theta=100000, pi=4, bt=False, mom=False...\n",
      "\tstep_eps: 0.1\n",
      "************************************************************\n",
      "\n",
      "\tstep_eps: 0.01\n",
      "************************************************************\n",
      "\n",
      "\tstep_eps: 1\n",
      "************************************************************\n",
      "\n",
      "************************************************************\n",
      "RUNNING: theta=100000, pi=6, bt=True, mom=True...\n",
      "\tstep_eps: 0.0001\n",
      "************************************************************\n",
      "\n",
      "************************************************************\n",
      "RUNNING: theta=100000, pi=6, bt=True, mom=False...\n",
      "\tstep_eps: 0.0001\n",
      "************************************************************\n",
      "\n",
      "************************************************************\n",
      "RUNNING: theta=100000, pi=6, bt=False, mom=True...\n",
      "\tstep_eps: 0.1\n",
      "************************************************************\n",
      "\n",
      "\tstep_eps: 0.01\n",
      "************************************************************\n",
      "\n",
      "\tstep_eps: 1\n",
      "************************************************************\n",
      "\n",
      "************************************************************\n",
      "RUNNING: theta=100000, pi=6, bt=False, mom=False...\n",
      "\tstep_eps: 0.1\n",
      "************************************************************\n",
      "\n",
      "\tstep_eps: 0.01\n",
      "************************************************************\n",
      "\n",
      "\tstep_eps: 1\n",
      "************************************************************\n",
      "\n",
      "************************************************************\n",
      "RUNNING: theta=1000000, pi=0.5, bt=True, mom=True...\n",
      "\tstep_eps: 0.0001\n",
      "************************************************************\n",
      "\n",
      "************************************************************\n",
      "RUNNING: theta=1000000, pi=0.5, bt=True, mom=False...\n",
      "\tstep_eps: 0.0001\n",
      "************************************************************\n",
      "\n",
      "************************************************************\n",
      "RUNNING: theta=1000000, pi=0.5, bt=False, mom=True...\n",
      "\tstep_eps: 0.1\n",
      "************************************************************\n",
      "\n",
      "\tstep_eps: 0.01\n",
      "************************************************************\n",
      "\n",
      "\tstep_eps: 1\n",
      "************************************************************\n",
      "\n",
      "************************************************************\n",
      "RUNNING: theta=1000000, pi=0.5, bt=False, mom=False...\n",
      "\tstep_eps: 0.1\n",
      "************************************************************\n",
      "\n",
      "\tstep_eps: 0.01\n",
      "************************************************************\n",
      "\n",
      "\tstep_eps: 1\n",
      "************************************************************\n",
      "\n",
      "************************************************************\n",
      "RUNNING: theta=1000000, pi=2, bt=True, mom=True...\n",
      "\tstep_eps: 0.0001\n",
      "************************************************************\n",
      "\n",
      "************************************************************\n",
      "RUNNING: theta=1000000, pi=2, bt=True, mom=False...\n",
      "\tstep_eps: 0.0001\n",
      "************************************************************\n",
      "\n",
      "************************************************************\n",
      "RUNNING: theta=1000000, pi=2, bt=False, mom=True...\n",
      "\tstep_eps: 0.1\n",
      "************************************************************\n",
      "\n",
      "\tstep_eps: 0.01\n",
      "************************************************************\n",
      "\n",
      "\tstep_eps: 1\n",
      "************************************************************\n",
      "\n",
      "************************************************************\n",
      "RUNNING: theta=1000000, pi=2, bt=False, mom=False...\n",
      "\tstep_eps: 0.1\n",
      "************************************************************\n",
      "\n",
      "\tstep_eps: 0.01\n",
      "************************************************************\n",
      "\n",
      "\tstep_eps: 1\n",
      "************************************************************\n",
      "\n",
      "************************************************************\n",
      "RUNNING: theta=1000000, pi=4, bt=True, mom=True...\n",
      "\tstep_eps: 0.0001\n",
      "************************************************************\n",
      "\n",
      "************************************************************\n",
      "RUNNING: theta=1000000, pi=4, bt=True, mom=False...\n",
      "\tstep_eps: 0.0001\n",
      "************************************************************\n",
      "\n",
      "************************************************************\n",
      "RUNNING: theta=1000000, pi=4, bt=False, mom=True...\n",
      "\tstep_eps: 0.1\n",
      "************************************************************\n",
      "\n",
      "\tstep_eps: 0.01\n",
      "************************************************************\n",
      "\n",
      "\tstep_eps: 1\n",
      "************************************************************\n",
      "\n",
      "************************************************************\n",
      "RUNNING: theta=1000000, pi=4, bt=False, mom=False...\n",
      "\tstep_eps: 0.1\n",
      "************************************************************\n",
      "\n",
      "\tstep_eps: 0.01\n",
      "************************************************************\n",
      "\n",
      "\tstep_eps: 1\n",
      "************************************************************\n",
      "\n",
      "************************************************************\n",
      "RUNNING: theta=1000000, pi=6, bt=True, mom=True...\n",
      "\tstep_eps: 0.0001\n",
      "************************************************************\n",
      "\n",
      "************************************************************\n",
      "RUNNING: theta=1000000, pi=6, bt=True, mom=False...\n",
      "\tstep_eps: 0.0001\n",
      "************************************************************\n",
      "\n",
      "************************************************************\n",
      "RUNNING: theta=1000000, pi=6, bt=False, mom=True...\n",
      "\tstep_eps: 0.1\n",
      "************************************************************\n",
      "\n",
      "\tstep_eps: 0.01\n",
      "************************************************************\n",
      "\n",
      "\tstep_eps: 1\n",
      "************************************************************\n",
      "\n",
      "************************************************************\n",
      "RUNNING: theta=1000000, pi=6, bt=False, mom=False...\n",
      "\tstep_eps: 0.1\n",
      "************************************************************\n",
      "\n",
      "\tstep_eps: 0.01\n",
      "************************************************************\n",
      "\n",
      "\tstep_eps: 1\n",
      "************************************************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# setup looping\n",
    "\n",
    "result = []\n",
    "\n",
    "thetas = [10**i for i in [-1, 1, 3, 5, 6]]\n",
    "pis = [0.5, 2, 4, 6]\n",
    "bts = [True, False]\n",
    "moms = [True, False]\n",
    "portfolio = np.array([1e-3]*3)\n",
    "\n",
    "max_iter = 20000\n",
    "m = len(portfolio)\n",
    "ret = ret_train.values\n",
    "\n",
    "\n",
    "loud = True\n",
    "\n",
    "for theta in thetas:\n",
    "    for pi in pis:\n",
    "          for bt in bts:\n",
    "                for mom in moms:\n",
    "                    if loud:\n",
    "                        print(\"*\"*60)\n",
    "                        print(f\"RUNNING: theta={theta}, pi={pi}, bt={bt}, mom={mom}...\")\n",
    "                    \n",
    "                    if bt:\n",
    "                        step_epss = [1e-4]\n",
    "                    else:\n",
    "                        step_epss = [0.1, 0.01, 1]\n",
    "                        step_epss\n",
    "                    for step_eps in step_epss:\n",
    "                        if loud:\n",
    "                            print(\"\\tstep_eps:\", step_eps)\n",
    "                        x_history = np.zeros((max_iter, m))\n",
    "                        f_history = np.zeros(max_iter)\n",
    "                        it, converged = run_grad_desc(\n",
    "                            x = portfolio, \n",
    "                            ret = ret, \n",
    "                            pi = pi, theta = theta, x_history = x_history, f_history = f_history,\n",
    "                            bt=bt, bt_a=0.5, bt_b=0.75, momentum=mom, mom_mu=0.8, \n",
    "                            max_iter = max_iter, step_eps=step_eps, \n",
    "                        )\n",
    "                        # portfolio_history = np.hstack([x_history, (1 - x_history.sum(axis = 1)).reshape(-1, 1)])\n",
    "                        final_objective = f_history[it-1]\n",
    "                        final_portfolio = x_history[it-1]\n",
    "                        result.append(\n",
    "                            {\n",
    "                                \"theta\": theta,\n",
    "                                \"pi\": pi,\n",
    "                                \"backtrack\": bt,\n",
    "                                \"momentum\": mom,\n",
    "                                \"step_size/step_eps\": step_eps,\n",
    "                                \"converged\": converged,\n",
    "                                \"converged_iter\": it,\n",
    "                                \"final_objective\": final_objective,\n",
    "                                \"final_portfolio\": final_portfolio\n",
    "                            }\n",
    "                        )\n",
    "                        if loud:\n",
    "                            print(\"*\"*60)\n",
    "                            print()\n",
    "\n",
    "\n",
    "result_df = pd.DataFrame(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model selection for each ($\\pi$, $\\theta$) pair:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>theta</th>\n",
       "      <th>pi</th>\n",
       "      <th>backtrack</th>\n",
       "      <th>momentum</th>\n",
       "      <th>step_size/step_eps</th>\n",
       "      <th>converged</th>\n",
       "      <th>converged_iter</th>\n",
       "      <th>final_objective</th>\n",
       "      <th>final_portfolio</th>\n",
       "      <th>normalized_portfolio</th>\n",
       "      <th>portfolio_return</th>\n",
       "      <th>normalized_objective</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>False</td>\n",
       "      <td>20000</td>\n",
       "      <td>-6.563347e-06</td>\n",
       "      <td>[0.0012437398702942965, -0.0011695824775389142...</td>\n",
       "      <td>[-0.0873816346815979, 0.08217154665800075, 1.0...</td>\n",
       "      <td>-0.000847</td>\n",
       "      <td>0.003417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>False</td>\n",
       "      <td>20000</td>\n",
       "      <td>5.738228e-07</td>\n",
       "      <td>[0.0005774073267974071, -1.2887796069270725e-0...</td>\n",
       "      <td>[-0.496970842033434, 0.011092444739184793, 1.4...</td>\n",
       "      <td>-0.002374</td>\n",
       "      <td>0.005960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>False</td>\n",
       "      <td>20000</td>\n",
       "      <td>2.003392e-08</td>\n",
       "      <td>[6.281451485940923e-06, -2.2785496781168815e-0...</td>\n",
       "      <td>[0.7427161983833196, -0.026941476162734516, 0....</td>\n",
       "      <td>0.001834</td>\n",
       "      <td>0.002369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>False</td>\n",
       "      <td>20000</td>\n",
       "      <td>1.759433e-06</td>\n",
       "      <td>[0.0019909723754727793, -0.0003196279004420772...</td>\n",
       "      <td>[1.7096073697270526, -0.2744579587832814, -0.4...</td>\n",
       "      <td>0.004759</td>\n",
       "      <td>0.001511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>10.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>False</td>\n",
       "      <td>20000</td>\n",
       "      <td>1.081306e-03</td>\n",
       "      <td>[0.002114037650003982, 0.005248424825926881, 0...</td>\n",
       "      <td>[0.17243760372114322, 0.4281029717193943, 0.39...</td>\n",
       "      <td>0.000614</td>\n",
       "      <td>0.088200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>10.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>False</td>\n",
       "      <td>20000</td>\n",
       "      <td>4.083929e-07</td>\n",
       "      <td>[4.125005997859856e-07, 3.9439868303129403e-07...</td>\n",
       "      <td>[0.17295424120403013, 0.1653644261146205, 0.66...</td>\n",
       "      <td>0.000186</td>\n",
       "      <td>0.171232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>10.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>False</td>\n",
       "      <td>20000</td>\n",
       "      <td>1.163904e-03</td>\n",
       "      <td>[-0.000887633798922205, -0.000558443039831269,...</td>\n",
       "      <td>[0.19077476730791007, 0.12002341630962333, 0.6...</td>\n",
       "      <td>0.000173</td>\n",
       "      <td>0.252985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>10.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>False</td>\n",
       "      <td>20000</td>\n",
       "      <td>4.633972e-02</td>\n",
       "      <td>[0.02873666100474581, 0.015476330762260383, 0....</td>\n",
       "      <td>[0.20288908111785425, 0.10926734065980831, 0.6...</td>\n",
       "      <td>0.000197</td>\n",
       "      <td>0.327172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>False</td>\n",
       "      <td>20000</td>\n",
       "      <td>1.271175e+01</td>\n",
       "      <td>[0.4432692130590657, 0.7842246914867097, 0.463...</td>\n",
       "      <td>[0.26213321544521084, 0.4637618267965461, 0.27...</td>\n",
       "      <td>0.000982</td>\n",
       "      <td>7.517267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>False</td>\n",
       "      <td>20000</td>\n",
       "      <td>3.599612e-03</td>\n",
       "      <td>[3.4170641964751845e-05, 3.494092658068692e-05...</td>\n",
       "      <td>[0.16275462433741075, 0.16642348673188143, 0.6...</td>\n",
       "      <td>0.000153</td>\n",
       "      <td>17.144937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>False</td>\n",
       "      <td>20000</td>\n",
       "      <td>4.156088e-03</td>\n",
       "      <td>[2.8584405436919832e-05, 2.0679579562709845e-0...</td>\n",
       "      <td>[0.17461586843204432, 0.12632701953940811, 0.6...</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>25.388632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>False</td>\n",
       "      <td>20000</td>\n",
       "      <td>2.994237e-03</td>\n",
       "      <td>[1.7109846233888805e-05, 1.0302240765701874e-0...</td>\n",
       "      <td>[0.1880107519460437, 0.11320569493210222, 0.69...</td>\n",
       "      <td>0.000152</td>\n",
       "      <td>32.902036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>100000.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>False</td>\n",
       "      <td>20000</td>\n",
       "      <td>2.763807e+03</td>\n",
       "      <td>[4.375292330967978, 0.9570960051861406, -0.463...</td>\n",
       "      <td>[0.8985891510044981, 0.1965665426839579, -0.09...</td>\n",
       "      <td>0.002737</td>\n",
       "      <td>567.625428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>100000.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>False</td>\n",
       "      <td>20000</td>\n",
       "      <td>1.416534e+04</td>\n",
       "      <td>[1.3439677563429981, 1.3750435606306874, 5.543...</td>\n",
       "      <td>[0.16266692750610418, 0.16642818262506207, 0.6...</td>\n",
       "      <td>0.000152</td>\n",
       "      <td>1714.499619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>100000.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>False</td>\n",
       "      <td>20000</td>\n",
       "      <td>3.760706e+04</td>\n",
       "      <td>[2.584667472881568, 1.8718392807959088, 10.355...</td>\n",
       "      <td>[0.17449452637676466, 0.12637049530853603, 0.6...</td>\n",
       "      <td>0.000127</td>\n",
       "      <td>2538.905139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>100000.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>False</td>\n",
       "      <td>20000</td>\n",
       "      <td>7.553715e+02</td>\n",
       "      <td>[0.04309408226487585, 0.026003253390580496, 0....</td>\n",
       "      <td>[0.18773317534260453, 0.11327943586887493, 0.6...</td>\n",
       "      <td>0.000152</td>\n",
       "      <td>3290.667169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>1000000.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>False</td>\n",
       "      <td>20000</td>\n",
       "      <td>2.774408e+08</td>\n",
       "      <td>[-29214.231764884156, -16806.695793001047, 97....</td>\n",
       "      <td>[0.6361495669729669, 0.36597136412860914, -0.0...</td>\n",
       "      <td>0.002110</td>\n",
       "      <td>6041.365593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>1000000.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>False</td>\n",
       "      <td>20000</td>\n",
       "      <td>1.705159e+04</td>\n",
       "      <td>[0.16178015932635287, 0.1655215797873712, 0.66...</td>\n",
       "      <td>[0.16266630766605153, 0.16642822170022153, 0.6...</td>\n",
       "      <td>0.000152</td>\n",
       "      <td>17144.993719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>1000000.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>False</td>\n",
       "      <td>20000</td>\n",
       "      <td>5.155865e+06</td>\n",
       "      <td>[35.43522242631488, 25.66266208422489, 141.976...</td>\n",
       "      <td>[0.17449381313503282, 0.12637075360776623, 0.6...</td>\n",
       "      <td>0.000127</td>\n",
       "      <td>25389.049222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>1000000.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>False</td>\n",
       "      <td>20000</td>\n",
       "      <td>9.177720e+06</td>\n",
       "      <td>[52.358771480843714, 31.593869220183578, 194.9...</td>\n",
       "      <td>[0.18773217954268134, 0.11327970006060047, 0.6...</td>\n",
       "      <td>0.000152</td>\n",
       "      <td>32906.680531</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         theta   pi  backtrack  momentum  step_size/step_eps  converged  \\\n",
       "1          0.1  0.5       True     False              0.0001      False   \n",
       "15         0.1  2.0      False     False              1.0000      False   \n",
       "22         0.1  4.0      False     False              0.0100      False   \n",
       "28         0.1  6.0      False      True              1.0000      False   \n",
       "34        10.0  0.5      False      True              0.1000      False   \n",
       "41        10.0  2.0       True     False              0.0001      False   \n",
       "55        10.0  4.0      False     False              1.0000      False   \n",
       "60        10.0  6.0      False      True              1.0000      False   \n",
       "69      1000.0  0.5      False     False              0.1000      False   \n",
       "73      1000.0  2.0       True     False              0.0001      False   \n",
       "81      1000.0  4.0       True     False              0.0001      False   \n",
       "89      1000.0  6.0       True     False              0.0001      False   \n",
       "99    100000.0  0.5      False      True              0.0100      False   \n",
       "107   100000.0  2.0      False      True              0.0100      False   \n",
       "115   100000.0  4.0      False      True              0.0100      False   \n",
       "120   100000.0  6.0       True      True              0.0001      False   \n",
       "132  1000000.0  0.5      False      True              1.0000      False   \n",
       "136  1000000.0  2.0       True      True              0.0001      False   \n",
       "150  1000000.0  4.0      False     False              0.0100      False   \n",
       "155  1000000.0  6.0      False      True              0.0100      False   \n",
       "\n",
       "     converged_iter  final_objective  \\\n",
       "1             20000    -6.563347e-06   \n",
       "15            20000     5.738228e-07   \n",
       "22            20000     2.003392e-08   \n",
       "28            20000     1.759433e-06   \n",
       "34            20000     1.081306e-03   \n",
       "41            20000     4.083929e-07   \n",
       "55            20000     1.163904e-03   \n",
       "60            20000     4.633972e-02   \n",
       "69            20000     1.271175e+01   \n",
       "73            20000     3.599612e-03   \n",
       "81            20000     4.156088e-03   \n",
       "89            20000     2.994237e-03   \n",
       "99            20000     2.763807e+03   \n",
       "107           20000     1.416534e+04   \n",
       "115           20000     3.760706e+04   \n",
       "120           20000     7.553715e+02   \n",
       "132           20000     2.774408e+08   \n",
       "136           20000     1.705159e+04   \n",
       "150           20000     5.155865e+06   \n",
       "155           20000     9.177720e+06   \n",
       "\n",
       "                                       final_portfolio  \\\n",
       "1    [0.0012437398702942965, -0.0011695824775389142...   \n",
       "15   [0.0005774073267974071, -1.2887796069270725e-0...   \n",
       "22   [6.281451485940923e-06, -2.2785496781168815e-0...   \n",
       "28   [0.0019909723754727793, -0.0003196279004420772...   \n",
       "34   [0.002114037650003982, 0.005248424825926881, 0...   \n",
       "41   [4.125005997859856e-07, 3.9439868303129403e-07...   \n",
       "55   [-0.000887633798922205, -0.000558443039831269,...   \n",
       "60   [0.02873666100474581, 0.015476330762260383, 0....   \n",
       "69   [0.4432692130590657, 0.7842246914867097, 0.463...   \n",
       "73   [3.4170641964751845e-05, 3.494092658068692e-05...   \n",
       "81   [2.8584405436919832e-05, 2.0679579562709845e-0...   \n",
       "89   [1.7109846233888805e-05, 1.0302240765701874e-0...   \n",
       "99   [4.375292330967978, 0.9570960051861406, -0.463...   \n",
       "107  [1.3439677563429981, 1.3750435606306874, 5.543...   \n",
       "115  [2.584667472881568, 1.8718392807959088, 10.355...   \n",
       "120  [0.04309408226487585, 0.026003253390580496, 0....   \n",
       "132  [-29214.231764884156, -16806.695793001047, 97....   \n",
       "136  [0.16178015932635287, 0.1655215797873712, 0.66...   \n",
       "150  [35.43522242631488, 25.66266208422489, 141.976...   \n",
       "155  [52.358771480843714, 31.593869220183578, 194.9...   \n",
       "\n",
       "                                  normalized_portfolio  portfolio_return  \\\n",
       "1    [-0.0873816346815979, 0.08217154665800075, 1.0...         -0.000847   \n",
       "15   [-0.496970842033434, 0.011092444739184793, 1.4...         -0.002374   \n",
       "22   [0.7427161983833196, -0.026941476162734516, 0....          0.001834   \n",
       "28   [1.7096073697270526, -0.2744579587832814, -0.4...          0.004759   \n",
       "34   [0.17243760372114322, 0.4281029717193943, 0.39...          0.000614   \n",
       "41   [0.17295424120403013, 0.1653644261146205, 0.66...          0.000186   \n",
       "55   [0.19077476730791007, 0.12002341630962333, 0.6...          0.000173   \n",
       "60   [0.20288908111785425, 0.10926734065980831, 0.6...          0.000197   \n",
       "69   [0.26213321544521084, 0.4637618267965461, 0.27...          0.000982   \n",
       "73   [0.16275462433741075, 0.16642348673188143, 0.6...          0.000153   \n",
       "81   [0.17461586843204432, 0.12632701953940811, 0.6...          0.000128   \n",
       "89   [0.1880107519460437, 0.11320569493210222, 0.69...          0.000152   \n",
       "99   [0.8985891510044981, 0.1965665426839579, -0.09...          0.002737   \n",
       "107  [0.16266692750610418, 0.16642818262506207, 0.6...          0.000152   \n",
       "115  [0.17449452637676466, 0.12637049530853603, 0.6...          0.000127   \n",
       "120  [0.18773317534260453, 0.11327943586887493, 0.6...          0.000152   \n",
       "132  [0.6361495669729669, 0.36597136412860914, -0.0...          0.002110   \n",
       "136  [0.16266630766605153, 0.16642822170022153, 0.6...          0.000152   \n",
       "150  [0.17449381313503282, 0.12637075360776623, 0.6...          0.000127   \n",
       "155  [0.18773217954268134, 0.11327970006060047, 0.6...          0.000152   \n",
       "\n",
       "     normalized_objective  \n",
       "1                0.003417  \n",
       "15               0.005960  \n",
       "22               0.002369  \n",
       "28               0.001511  \n",
       "34               0.088200  \n",
       "41               0.171232  \n",
       "55               0.252985  \n",
       "60               0.327172  \n",
       "69               7.517267  \n",
       "73              17.144937  \n",
       "81              25.388632  \n",
       "89              32.902036  \n",
       "99             567.625428  \n",
       "107           1714.499619  \n",
       "115           2538.905139  \n",
       "120           3290.667169  \n",
       "132           6041.365593  \n",
       "136          17144.993719  \n",
       "150          25389.049222  \n",
       "155          32906.680531  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def normalize_portfolio(x:np.ndarray):\n",
    "    return x / x.sum()\n",
    "\n",
    "result_df[\"normalized_portfolio\"] = result_df.final_portfolio.apply(normalize_portfolio)\n",
    "\n",
    "result_df[\"portfolio_return\"] = result_df.normalized_portfolio.apply(\n",
    "    lambda x : benchmark(ret_test, price_test, x)[1]\n",
    ")\n",
    "\n",
    "result_df[\"normalized_objective\"] = result_df[[\"normalized_portfolio\", \"pi\", \"theta\"]].apply(\n",
    "    lambda x: evalfunc(\n",
    "        x[0], \n",
    "        ret_train, \n",
    "        x[1], \n",
    "        x[2]),\n",
    "    axis = 1\n",
    ")\n",
    "\n",
    "groupby_best = result_df.loc[\n",
    "    :,\n",
    "    [\"theta\", \"pi\", \"normalized_objective\"]\n",
    "].groupby([\"theta\", \"pi\"]).idxmin().values.flatten()\n",
    "\n",
    "groupby_best = result_df.iloc[groupby_best]\n",
    "\n",
    "groupby_best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35.0% of best models used backtrack.\n",
      "\n",
      "50.0% of best models used momentum.\n",
      "\n",
      "For best constant step size models, the most common step size is 0.01.\n"
     ]
    }
   ],
   "source": [
    "share_bt = groupby_best.backtrack.mean()\n",
    "share_mom = groupby_best.momentum.mean()\n",
    "\n",
    "most_common_constant_stepsize = groupby_best[groupby_best.backtrack == False][\"step_size/step_eps\"].mode().values[0]\n",
    "\n",
    "\n",
    "print(f\"{round(share_bt*100, 2)}% of best models used backtrack.\")\n",
    "print()\n",
    "print(f\"{round(share_mom*100, 2)}% of best models used momentum.\")\n",
    "print()\n",
    "print(f\"For best constant step size models, the most common step size is {most_common_constant_stepsize}.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
